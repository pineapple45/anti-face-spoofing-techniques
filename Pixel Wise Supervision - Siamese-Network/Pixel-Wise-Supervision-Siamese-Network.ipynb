{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j89U_0blc58u"
      },
      "source": [
        "# PIXEL-WISE-SUPERVISION-SIAMESE-NETWORK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CHQeWTdUc580",
        "outputId": "252b3b16-81d6-4844-d80e-678b35f302ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lRs1-_mdc583",
        "outputId": "ece41da3-f9ec-4452-d596-89329212c0f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/BTP/pixel-wise-supervision-siamese-network\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/BTP/pixel-wise-supervision-siamese-network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rYPn-Mrlc584"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from datetime import datetime\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow import keras\n",
        "\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9nZBZwpCc586",
        "outputId": "df110bc6-c1d9-47f4-c5c5-e0b0aad3b1a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "    raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "v32cEmY6c587"
      },
      "outputs": [],
      "source": [
        "epochs = 50\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.00006)\n",
        "binary_cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "# base_dir = \".\"\n",
        "base_dir = \"/content/drive/MyDrive/BTP/pixel-wise-supervision-siamese-network\"\n",
        "\n",
        "stamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "logdir = os.path.join(base_dir, 'logs/func/%s' % stamp)\n",
        "writer = tf.summary.create_file_writer(logdir)\n",
        "\n",
        "scalar_logdir = os.path.join(base_dir, 'logs/scalars/%s' % stamp)\n",
        "file_writer = tf.summary.create_file_writer(scalar_logdir + \"/metrics\")\n",
        "\n",
        "checkpoint_path = os.path.join(base_dir, 'logs/model/pixel-wise-supervision-siamese')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2am6nQi8c588"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "The following code cell is taken from the source code of keras_vggface.\n",
        "I tried using the preprocess_input function provided by tf.keras but they provide different results.\n",
        "To my knowledge, it seems that the mean values which are subtracted in each image are different.\n",
        "'''\n",
        "K = tf.keras.backend\n",
        "\n",
        "def preprocess_input(x, data_format=None, version=1):\n",
        "    x_temp = np.copy(x)\n",
        "    if data_format is None:\n",
        "        data_format = K.image_data_format()\n",
        "    assert data_format in {'channels_last', 'channels_first'}\n",
        "\n",
        "    if version == 1:\n",
        "        if data_format == 'channels_first':\n",
        "            x_temp = x_temp[:, ::-1, ...]\n",
        "            x_temp[:, 0, :, :] -= 93.5940\n",
        "            x_temp[:, 1, :, :] -= 104.7624\n",
        "            x_temp[:, 2, :, :] -= 129.1863\n",
        "        else:\n",
        "            x_temp = x_temp[..., ::-1]\n",
        "            x_temp[..., 0] -= 93.5940\n",
        "            x_temp[..., 1] -= 104.7624\n",
        "            x_temp[..., 2] -= 129.1863\n",
        "\n",
        "    elif version == 2:\n",
        "        if data_format == 'channels_first':\n",
        "            x_temp = x_temp[:, ::-1, ...]\n",
        "            x_temp[:, 0, :, :] -= 91.4953\n",
        "            x_temp[:, 1, :, :] -= 103.8827\n",
        "            x_temp[:, 2, :, :] -= 131.0912\n",
        "        else:\n",
        "            x_temp = x_temp[..., ::-1]\n",
        "            x_temp[..., 0] -= 91.4953\n",
        "            x_temp[..., 1] -= 103.8827\n",
        "            x_temp[..., 2] -= 131.0912\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    return x_temp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def curate_dataset(dataset_path):\n",
        "        with open(os.path.join(dataset_path, 'list.txt'), 'r') as f:\n",
        "            dataset = {}\n",
        "            image_list = f.read().split()\n",
        "            print('Total images: ',len(image_list))\n",
        "            for image in image_list:\n",
        "                folder_name, class_type ,file_name = image.split('/')\n",
        "                if folder_name in dataset.keys():\n",
        "                  if class_type in dataset[folder_name].keys():\n",
        "                    dataset[folder_name][class_type].append(file_name)\n",
        "                  else:\n",
        "                    dataset[folder_name][class_type] = [file_name]\n",
        "                else:\n",
        "                    dataset[folder_name] = {class_type: [file_name]}\n",
        "        return dataset\n",
        "\n",
        "dataset = curate_dataset('./dataset')\n",
        "print(dataset.keys()) \n",
        "print(len(dataset['client016']['spoof']))\n",
        "print(len(dataset['client016']['real']))"
      ],
      "metadata": {
        "id": "rnkeGG-kwuT6",
        "outputId": "f1fbd52c-f58a-4cb8-d0a0-85dbfef72876",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images:  91705\n",
            "dict_keys(['client016', 'client025', 'client108', 'client006', 'client027', 'client103', 'client004', 'client008', 'client018', 'client007', 'client012', 'client110', 'client002', 'client001', 'client105'])\n",
            "4538\n",
            "1500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "m0S3NV7lc58_"
      },
      "outputs": [],
      "source": [
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, dataset_path, batch_size=32, shuffle=True):\n",
        "        self.dataset = self.curate_dataset(dataset_path)\n",
        "        self.dataset_path = dataset_path\n",
        "        self.shuffle = shuffle\n",
        "        self.batch_size =batch_size\n",
        "        self.no_of_people = len(list(self.dataset.keys()))\n",
        "        self.on_epoch_end()\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        people = list(self.dataset.keys())[index * self.batch_size: (index + 1) * self.batch_size]\n",
        "        P = []\n",
        "        A = []\n",
        "        N = []\n",
        "        \n",
        "        for person in people:\n",
        "            anchor_index = random.randint(0, len(self.dataset[person]['real'])-1)\n",
        "            a = self.get_image(person, 'real' ,anchor_index)\n",
        "            \n",
        "            positive_index = random.randint(0, len(self.dataset[person]['real'])-1)\n",
        "            while positive_index == anchor_index:\n",
        "                positive_index = random.randint(0, len(self.dataset[person]['real'])-1)\n",
        "            p = self.get_image(person, 'real' ,positive_index)\n",
        "            \n",
        "            negative_index = random.randint(0, len(self.dataset[person]['spoof'])-1)\n",
        "            n = self.get_image(person, 'spoof',negative_index)\n",
        "            \n",
        "            P.append(p)\n",
        "            A.append(a)\n",
        "            N.append(n)\n",
        "        A = np.asarray(A)\n",
        "        N = np.asarray(N)\n",
        "        P = np.asarray(P)\n",
        "        return [A, P, N]\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.no_of_people // self.batch_size\n",
        "        \n",
        "    def curate_dataset(self,dataset_path):\n",
        "        with open(os.path.join(dataset_path, 'list.txt'), 'r') as f:\n",
        "            dataset = {}\n",
        "            image_list = f.read().split()\n",
        "            print('Total images: ',len(image_list))\n",
        "            for image in image_list:\n",
        "                folder_name, class_type ,file_name = image.split('/')\n",
        "                if folder_name in dataset.keys():\n",
        "                  if class_type in dataset[folder_name].keys():\n",
        "                    dataset[folder_name][class_type].append(file_name)\n",
        "                  else:\n",
        "                    dataset[folder_name][class_type] = [file_name]\n",
        "                else:\n",
        "                    dataset[folder_name] = {class_type: [file_name]}\n",
        "        return dataset\n",
        "    \n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            keys = list(self.dataset.keys())\n",
        "            random.shuffle(keys)\n",
        "            dataset_ =  {}\n",
        "            for key in keys:\n",
        "                dataset_[key] = self.dataset[key]\n",
        "            self.dataset = dataset_\n",
        "            \n",
        "    def get_image(self, person, class_type, index):\n",
        "        # print(os.path.join(self.dataset_path, os.path.join('images/' + person, self.dataset[person][index])))\n",
        "        img = cv2.imread(os.path.join(self.dataset_path, os.path.join('images/' + person+f'/{class_type}', self.dataset[person][class_type][index])))\n",
        "        img = cv2.resize(img, (256, 256))\n",
        "        img = np.asarray(img, dtype=np.float64)\n",
        "        img = preprocess_input(img)\n",
        "        return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kjsp7-9Oc59E"
      },
      "outputs": [],
      "source": [
        "# index = 4\n",
        "# cv2.imshow(\"Positive\", p[index])\n",
        "# cv2.imshow(\"Negative\", n[index])\n",
        "# cv2.imshow(\"Anchor\", a[index])\n",
        "# cv2.waitKey(0)\n",
        "# cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "kA4fKJX6c59F"
      },
      "outputs": [],
      "source": [
        "vggface = tf.keras.models.Sequential()\n",
        "vggface.add(tf.keras.layers.Convolution2D(64, (3, 3), activation='relu', padding=\"SAME\", input_shape=(256,256, 3)))\n",
        "vggface.add(tf.keras.layers.Convolution2D(64, (3, 3), activation='relu', padding=\"SAME\"))\n",
        "vggface.add(tf.keras.layers.MaxPooling2D((2,2), strides=(2,2)))\n",
        " \n",
        "vggface.add(tf.keras.layers.Convolution2D(128, (3, 3), activation='relu', padding=\"SAME\"))\n",
        "vggface.add(tf.keras.layers.Convolution2D(128, (3, 3), activation='relu', padding=\"SAME\"))\n",
        "vggface.add(tf.keras.layers.MaxPooling2D((2,2), strides=(2,2)))\n",
        " \n",
        "vggface.add(tf.keras.layers.Convolution2D(256, (3, 3), activation='relu', padding=\"SAME\"))\n",
        "vggface.add(tf.keras.layers.Convolution2D(256, (3, 3), activation='relu', padding=\"SAME\"))\n",
        "vggface.add(tf.keras.layers.Convolution2D(256, (3, 3), activation='relu', padding=\"SAME\"))\n",
        "vggface.add(tf.keras.layers.MaxPooling2D((2,2), strides=(2,2)))\n",
        " \n",
        "vggface.add(tf.keras.layers.Convolution2D(512, (3, 3), activation='relu', padding=\"SAME\"))\n",
        "vggface.add(tf.keras.layers.Convolution2D(512, (3, 3), activation='relu', padding=\"SAME\"))\n",
        "vggface.add(tf.keras.layers.Convolution2D(512, (3, 3), activation='relu', padding=\"SAME\"))\n",
        "vggface.add(tf.keras.layers.MaxPooling2D((2,2), strides=(2,2)))\n",
        " \n",
        "vggface.add(tf.keras.layers.Convolution2D(512, (3, 3), activation='relu', padding=\"SAME\"))\n",
        "vggface.add(tf.keras.layers.Convolution2D(512, (3, 3), activation='relu', padding=\"SAME\"))\n",
        "vggface.add(tf.keras.layers.Convolution2D(512, (3, 3), activation='relu', padding=\"SAME\"))\n",
        "vggface.add(tf.keras.layers.MaxPooling2D((2,2), strides=(2,2)))\n",
        "\n",
        "vggface.add(tf.keras.layers.Flatten())\n",
        "\n",
        "vggface.add(tf.keras.layers.Dense(4096, activation='relu'))\n",
        "vggface.add(tf.keras.layers.Dropout(0.5))\n",
        "vggface.add(tf.keras.layers.Dense(4096, activation='relu'))\n",
        "vggface.add(tf.keras.layers.Dropout(0.5))\n",
        "vggface.add(tf.keras.layers.Dense(2622, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hB_Eld0Lc59H"
      },
      "outputs": [],
      "source": [
        "vggface.load_weights(os.path.join(base_dir, 'vgg_face_weights.h5'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "IJYQpsoWc59I"
      },
      "outputs": [],
      "source": [
        "vggface.pop()\n",
        "vggface.add(tf.keras.layers.Dense(128, use_bias=False))\n",
        "\n",
        "for layer in vggface.layers[:-2]:\n",
        "    layer.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "nNVBKvudc59I"
      },
      "outputs": [],
      "source": [
        "class SiameseNetwork(tf.keras.Model):\n",
        "    def __init__(self, vgg_face):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "        self.vgg_face = vgg_face\n",
        "        \n",
        "    @tf.function\n",
        "    def call(self, inputs):\n",
        "        image_1, image_2, image_3 =  inputs\n",
        "        with tf.name_scope(\"Anchor\") as scope:\n",
        "            feature_1 = self.vgg_face(image_1)\n",
        "            feature_1 = tf.math.l2_normalize(feature_1, axis=-1)\n",
        "        with tf.name_scope(\"Positive\") as scope:\n",
        "            feature_2 = self.vgg_face(image_2)\n",
        "            feature_2 = tf.math.l2_normalize(feature_2, axis=-1)\n",
        "        with tf.name_scope(\"Negative\") as scope:\n",
        "            feature_3 = self.vgg_face(image_3)\n",
        "            feature_3 = tf.math.l2_normalize(feature_3, axis=-1)\n",
        "        return [feature_1, feature_2, feature_3]\n",
        "    \n",
        "    @tf.function\n",
        "    def get_features(self, inputs):\n",
        "        return tf.math.l2_normalize(self.vgg_face(inputs), axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "QpJinqQMc59J"
      },
      "outputs": [],
      "source": [
        "model = SiameseNetwork(vggface)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "hU78wdNFc59K"
      },
      "outputs": [],
      "source": [
        "def loss_function(x, alpha = 0.2):\n",
        "    # Triplet Loss function.\n",
        "    anchor,positive,negative = x\n",
        "    # distance between the anchor and the positive\n",
        "    pos_dist = K.sum(K.square(anchor-positive),axis=1)\n",
        "    # distance between the anchor and the negative\n",
        "    neg_dist = K.sum(K.square(anchor-negative),axis=1)\n",
        "    # compute loss\n",
        "    basic_loss = pos_dist-neg_dist+alpha\n",
        "    loss = K.mean(K.maximum(basic_loss,0.0))\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "j--mda5Hc59K"
      },
      "outputs": [],
      "source": [
        "def train(X):\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred = model(X)\n",
        "        loss = loss_function(y_pred)\n",
        "    grad = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grad, model.trainable_variables))\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "2viXvp2Zc59L"
      },
      "outputs": [],
      "source": [
        "tf.summary.trace_on(graph=True, profiler=False)\n",
        "output = model([tf.zeros((32,256,256,3)), tf.zeros((32,256,256,3)), tf.zeros((32,256,256,3))])\n",
        "with writer.as_default():\n",
        "    tf.summary.trace_export(name=\"my_func_trace\", step=0, profiler_outdir=logdir)\n",
        "    \n",
        "tf.summary.trace_off()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "cqffSFvjc59L",
        "outputId": "0337be72-eaff-4094-fa5f-5ab170461ede",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images:  91705\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-0ebb7b9604b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./dataset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_generator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-37-3b8cac99cdb1>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mperson\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpeople\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0manchor_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperson\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'real'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'real'\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0manchor_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mpositive_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperson\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'real'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-3b8cac99cdb1>\u001b[0m in \u001b[0;36mget_image\u001b[0;34m(self, person, class_type, index)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# print(os.path.join(self.dataset_path, os.path.join('images/' + person, self.dataset[person][index])))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'images/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mperson\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34mf'/{class_type}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperson\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.1.2) /io/opencv/modules/imgproc/src/resize.cpp:3720: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n"
          ]
        }
      ],
      "source": [
        "data_generator = DataGenerator(dataset_path='./dataset/train')\n",
        "print(data_generator[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Pr4xG51nc59M",
        "outputId": "71f91111-292e-4dc7-c706-c9a8d9ff983e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images:  91705\n"
          ]
        }
      ],
      "source": [
        "data_curated = data_generator.curate_dataset(dataset_path='./dataset/train')\n",
        "# print(len(data_curated['n000009']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "JSzXjuErc59M",
        "outputId": "a8fc45de-1cb0-4bce-a123-f2d92dfd33ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-6b09cf4788cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-3b8cac99cdb1>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mperson\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpeople\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0manchor_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperson\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'real'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'real'\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0manchor_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mpositive_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperson\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'real'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-3b8cac99cdb1>\u001b[0m in \u001b[0;36mget_image\u001b[0;34m(self, person, class_type, index)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# print(os.path.join(self.dataset_path, os.path.join('images/' + person, self.dataset[person][index])))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'images/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mperson\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34mf'/{class_type}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperson\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.1.2) /io/opencv/modules/imgproc/src/resize.cpp:3720: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n"
          ]
        }
      ],
      "source": [
        "a, p, n = data_generator[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bZqQbWic59N"
      },
      "outputs": [],
      "source": [
        "# index = 1\n",
        "# cv2.imshow(\"Positive\", p[index])\n",
        "# cv2.imshow(\"Negative\", n[index])\n",
        "# cv2.imshow(\"Anchor\", a[index])\n",
        "# cv2.waitKey(0)\n",
        "# cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlfXRNCCc59O"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir /content/drive/My\\ Drive/pixel-wise-supervision-siamese-network/logs/\n",
        "# %tensorboard --logdir ./logs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPqdlc9fc59O"
      },
      "outputs": [],
      "source": [
        "checkpoint = tf.train.Checkpoint(model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgvWlG1xc59P"
      },
      "outputs": [],
      "source": [
        "losses = []\n",
        "accuracy = []\n",
        "\n",
        "no_of_batches = data_generator.__len__()\n",
        "for i in range(1, epochs+1, 1):\n",
        "    loss = 0\n",
        "    with tqdm(total=no_of_batches) as pbar:\n",
        "        \n",
        "        description = \"Epoch \" + str(i) + \"/\" + str(epochs)\n",
        "        pbar.set_description_str(description)\n",
        "        \n",
        "        for j in range(no_of_batches):\n",
        "            data = data_generator[j]\n",
        "            temp = train(data)\n",
        "            loss += temp\n",
        "            \n",
        "            pbar.update()\n",
        "            print_statement = \"Loss :\" + str(temp.numpy())\n",
        "            pbar.set_postfix_str(print_statement)\n",
        "        \n",
        "        loss /= no_of_batches\n",
        "        losses.append(loss.numpy())\n",
        "        with file_writer.as_default():\n",
        "            tf.summary.scalar('Loss', data=loss.numpy(), step=i)\n",
        "            \n",
        "        print_statement = \"Loss :\" + str(loss.numpy())\n",
        "        \n",
        "        pbar.set_postfix_str(print_statement)\n",
        "\n",
        "checkpoint.save(checkpoint_path)\n",
        "print(\"Checkpoint Saved\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "Pixel-Wise-Supervision-Siamese-Network.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}